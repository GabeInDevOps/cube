---
redirect_from:
  - /caching/running-in-production
---

# Running in production

## Architecture

Cube Store is designed to run as a cluster of multiple nodes. A Cube Store cluster
consists of at least one Router and one or more Worker nodes.

<div style={{ textAlign: "center" }}>
  <img
    alt="Cube Store cluster with Cube"
    src="https://ucarecdn.com/28747c64-bc0e-4f2c-a438-e5a112c68436/"
    style={{ border: "none" }}
    width="100%"
  />
</div>

Here's how Cube Store works:
- Cube sends queries to the Cube Store Router node.
- **Cube Store Router** handles incoming connections, manages database metadata,
serves simple queries, and distributes the queries over Cube Store Workers.
- Multiple **Cube Store Workers** execute the queries and return the results to
the Router, which in turn combines and returns the results to Cube.

<WarningBox>

When Cube is run in the [development mode][ref-dev-mode], Cube Store will run
in-process, in a single-instance mode. However, this is absolutely unsuitable
for production deployments. For high concurrency and data throughput, we
strongly recommend running Cube Store as a cluster of multiple nodes.

</WarningBox>

## Configuration

### Docker images

Cube Store is shipped as the single [`cubejs/cubestore` Docker
image][link-cubestore-docker-image]. It can be configured to run as either
a Router node or a Worker node. See the section on [environment
variables](#environment-variables) for details.

You can run a Cube Store node using Docker with the following command:

```bash
docker run -p 3030:3030 cubejs/cubestore
```

<InfoBox>

Using Windows? We strongly recommend using [WSL2 for Windows 10][link-wsl2]
to run Docker commands.

</InfoBox>

A sample Docker Compose configuration with a single Cube Store Router node
and two Cube Store Worker nodes:

```yaml
version: "2.2"

services:
  cubestore_router:
    restart: always
    image: cubejs/cubestore:latest
    environment:
      - CUBESTORE_SERVER_NAME=cubestore_router:9999
      - CUBESTORE_META_PORT=9999
      - CUBESTORE_WORKERS=cubestore_worker_1:9001,cubestore_worker_2:9001
      - CUBESTORE_REMOTE_DIR=/cube/data
    volumes:
      - .cubestore:/cube/data

  cubestore_worker_1:
    restart: always
    image: cubejs/cubestore:latest
    environment:
      - CUBESTORE_SERVER_NAME=cubestore_worker_1:9001
      - CUBESTORE_WORKER_PORT=9001
      - CUBESTORE_META_ADDR=cubestore_router:9999
      - CUBESTORE_WORKERS=cubestore_worker_1:9001,cubestore_worker_2:9001
      - CUBESTORE_REMOTE_DIR=/cube/data
    depends_on:
      - cubestore_router
    volumes:
      - .cubestore:/cube/data

  cubestore_worker_2:
    restart: always
    image: cubejs/cubestore:latest
    environment:
      - CUBESTORE_SERVER_NAME=cubestore_worker_2:9001
      - CUBESTORE_WORKER_PORT=9001
      - CUBESTORE_META_ADDR=cubestore_router:9999
      - CUBESTORE_WORKERS=cubestore_worker_1:9001,cubestore_worker_2:9001
      - CUBESTORE_REMOTE_DIR=/cube/data
    depends_on:
      - cubestore_router
    volumes:
      - .cubestore:/cube/data

  cube:
    image: cubejs/cube:latest
    ports:
      - 4000:4000
    environment:
      - CUBEJS_CUBESTORE_HOST=cubestore_router
    depends_on:
      - cubestore_router
    volumes:
      - .:/cube/conf
```

Note that we're specifying `CUBEJS_CUBESTORE_HOST` to let Cube know
where the Cube Store Router is running.

### Environment variables

Cube Store is configured via environment variables. Check the `CUBESTORE_*`
environment variables in the [environment variables reference][ref-config-env].

The configuration required for each node can be found in the table below:

| Environment Variable    | Specify on Router? | Specify on Worker? |
| ----------------------- | ------------------ | ------------------ |
| `CUBESTORE_SERVER_NAME` | Yes                | Yes                |
| `CUBESTORE_META_PORT`   | Yes                | -                  |
| `CUBESTORE_WORKERS`     | Yes                | Yes                |
| `CUBESTORE_WORKER_PORT` | -                  | Yes                |
| `CUBESTORE_META_ADDR`   | -                  | Yes                |

`CUBESTORE_WORKERS` and `CUBESTORE_META_ADDR` variables should be set with
stable IP addresses, which should not change. You can use stable DNS names and put
load balancers in front of your worker and router instances to fulfill this
requirement in environments where stable IP addresses can't be guaranteed.

## Scaling

Since the [storage layer](#storage) is decoupled from the query processing
engine, you can horizontally scale your Cube Store cluster for as much
concurrency as you need. To do so, increase the number of Cube Store Worker
nodes according to the recommendations below.

### Partitions

When ingesting the data into the [storage layer](#storage), Cube Store distributes
and stores it as a number of physical partitions (not to be confused with time-based
logical [partitions][ref-caching-partitioning] of pre-aggregations).

Each partition is represented as a single compressed Parquet file. The maximum size
of this Parquet file is set by the `CUBESTORE_MAX_PARTITION_SIZE` environment variable.

Cube Store querying performance is optimal when the count of partitions targeted
by a single query is less than or equal to the Cube Store Worker count; also, for
optimal performance, the partition size should not exceed 2 megabytes.

For example, if a query targets 100,000 rows of raw data, it would probably amount
to 30 MB of uncompressed or 3 MB of compressed data in Parquet format. In that case,
you need at least 2 Cube Store Workers to achieve optimal performance.

<InfoBox>

In Cube Cloud, `CUBESTORE_MAX_PARTITION_SIZE` is set to 2 MB. It means that,
when using [16 Cube Store Workers][ref-cube-cloud-limits], you can ingest and query
with optimal performance datasets of up to 32 MB of compressed data, or approximately
300 MB of uncompressed data; usually, datasets of this size contain roughly a million
rows of raw data.

When building pre-aggregations that exceed this limit, you will get an error message.
In that case, [contact us](https://cube.dev/contact) to discuss your use case
and fine tune the limits.

</InfoBox>

You can use [`EXPLAIN` and `EXPLAIN ANALYZE` SQL queries][ref-cube-store-explain]
to check how many partitions would be used for a specific query.

### Resources

Resources required for the main node and workers can vary depending on the
configuration. With default settings, you should expect to allocate at least 4
CPUs and up to 8GB per main or worker node.

## Replication and High Availability

The open-source version of Cube Store doesn't support replicating any of its
nodes. The router node and every worker node should always have only one
instance copy if served behind the load balancer or service address. Replication
will lead to undefined behavior of the cluster, including connection errors and
data loss. If any cluster node is down, it'll lead to a complete cluster outage.
If Cube Store replication and high availability are required, please consider
using Cube Cloud.

## Storage

Cube Store cluster uses both persistent and scratch storage.

### Persistent storage

Cube Store makes use of a separate storage layer for storing metadata as well as
for persisting pre-aggregations as Parquet files.

Cube Store [can be configured][ref-config-env] to use either AWS S3 or
Google Cloud Storage (GCS) as persistent storage. If desired, local path on
the server can also be used in case all Cube Store cluster nodes are
co-located on a single machine.

<InfoBox>

Cube Store can only use one type of remote storage at runtime.

</InfoBox>

<WarningBox>

Cube Store requires strong consistency guarantees from underlying distributed
storage. AWS S3, Google Cloud Storage, and Azure Blob Storage (Cube Cloud only)
are the only known implementations that provide strong consistency. Using other
implementations in production is discouraged and can lead to consistency and
data corruption errors.

</WarningBox>

A simplified example using AWS S3 might look like:

```yaml
version: "2.2"

services:
  cubestore_router:
    image: cubejs/cubestore:latest
    environment:
      - CUBESTORE_SERVER_NAME=cubestore_router:9999
      - CUBESTORE_META_PORT=9999
      - CUBESTORE_WORKERS=cubestore_worker_1:9001
      - CUBESTORE_S3_BUCKET=<BUCKET_NAME_IN_S3>
      - CUBESTORE_S3_REGION=<BUCKET_REGION_IN_S3>
      - CUBESTORE_AWS_ACCESS_KEY_ID=<AWS_ACCESS_KEY_ID>
      - CUBESTORE_AWS_SECRET_ACCESS_KEY=<AWS_SECRET_ACCESS_KEY>

  cubestore_worker_1:
    image: cubejs/cubestore:latest
    environment:
      - CUBESTORE_SERVER_NAME=cubestore_worker_1:9001
      - CUBESTORE_WORKER_PORT=9001
      - CUBESTORE_META_ADDR=cubestore_router:9999
      - CUBESTORE_WORKERS=cubestore_worker_1:9001
      - CUBESTORE_S3_BUCKET=<BUCKET_NAME_IN_S3>
      - CUBESTORE_S3_REGION=<BUCKET_REGION_IN_S3>
      - CUBESTORE_AWS_ACCESS_KEY_ID=<AWS_ACCESS_KEY_ID>
      - CUBESTORE_AWS_SECRET_ACCESS_KEY=<AWS_SECRET_ACCESS_KEY>
    depends_on:
      - cubestore_router
```

Note that you canâ€™t use the same bucket as an export bucket and persistent
storage for Cube Store. It's recommended to use two separate buckets. 

### Scratch storage

Separately from persistent storage, Cube Store requires local scratch space
to warm up partitions by downloading Parquet files before querying them.

By default, this folder should be mounted to `.cubestore/data` inside the
container and can be configured by `CUBESTORE_DATA_DIR` environment variable.
It is advised to use local SSDs for this scratch space to maximize querying
performance.

### AWS

Cube Store can retrieve security credentials from instance metadata
automatically. This means you can skip defining the
`CUBESTORE_AWS_ACCESS_KEY_ID` and `CUBESTORE_AWS_SECRET_ACCESS_KEY` environment
variables.

<WarningBox>

Cube Store currently does not take the key expiration time returned from
instance metadata into account; instead the refresh duration for the key is
defined by `CUBESTORE_AWS_CREDS_REFRESH_EVERY_MINS`, which is set to `180` by
default.

</WarningBox>

### Garbage collection

Cleanup isnâ€™t done in export buckets; however, it's done in the persistent
storage of Cube Store. The default time-to-live (TTL) for orphaned
pre-aggregation tables is one day.

Refresh worker should be able to finish pre-aggregation refresh before
garbage collection starts. It means that all pre-aggregation partitions
should be built before any tables are removed.

## Security

Cube Store currently does not have any in-built authentication mechanisms. For
this reason, we recommend running your Cube Store cluster on a network that only
allows requests from the Cube deployment.

[link-cubestore-docker-image]: https://hub.docker.com/r/cubejs/cubestore
[link-wsl2]: https://docs.microsoft.com/en-us/windows/wsl/install-win10

[ref-caching-partitioning]: /product/caching/using-pre-aggregations#partitioning
[ref-config-env]: /reference/configuration/environment-variables
[ref-dev-mode]: /product/configuration#development-mode
[ref-cube-cloud-limits]: /product/deployment/cloud/limits#resources
[ref-cube-store-explain]: /product/caching/using-pre-aggregations#explain-queries